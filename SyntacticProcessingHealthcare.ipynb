{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f28a0ed-efd5-4990-a353-e8a6e04070bb",
   "metadata": {},
   "source": [
    "### üöÄ Stage 1 ‚Äì Data Preprocessing\n",
    "\n",
    "- Task 1.1: Construct Proper Sentences\n",
    "- Task 1.2 & 1.3: Count Sentences and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6a7ce8-a845-4137-92cc-7ce4e144647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb014923-154c-4307-ab10-8c5766cc17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01909a01-6501-468e-9dc6-2dae602fa602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1: Task 1 - Constructs proper sentences from individual words and prints five sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33aeb567-0c54-4b48-9371-78c801e9cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load, process the file and provide sentence\n",
    "def load_datafile(input_file):\n",
    "    i_file=open(input_file, 'r')\n",
    "    file_name=i_file.readlines()\n",
    "    i_file.close()\n",
    "    output_list=[]\n",
    "    full_sentence=\"\"\n",
    "    for each_word in file_name:\n",
    "        each_word=each_word.strip()\n",
    "        if each_word==\"\":\n",
    "            output_list.append(full_sentence) \n",
    "            full_sentence=\"\"\n",
    "        else:\n",
    "            if full_sentence:\n",
    "                full_sentence+=\" \"+each_word\n",
    "            else:\n",
    "                full_sentence=each_word\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fe1d10-18bf-4050-bb95-96ef97d7def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data file\n",
    "train_sentences=load_datafile('train_sent')\n",
    "train_labels=load_datafile('train_label')\n",
    "test_sentences=load_datafile('test_sent')\n",
    "test_labels=load_datafile('test_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afaa8a48-d414-4a26-8766-e7059cc29506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Train Sentence 1: All live births > or = 23 weeks at the University of Vermont in 1995 ( n = 2395 ) were retrospectively analyzed for delivery route , indication for cesarean , gestational age , parity , and practice group ( to reflect risk status )\n",
      "‚úîÔ∏è Train Label 1: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Train Sentence 2: The total cesarean rate was 14.4 % ( 344 of 2395 ) , and the primary rate was 11.4 % ( 244 of 2144 )\n",
      "‚úîÔ∏è Train Label 2: O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Train Sentence 3: Abnormal presentation was the most common indication ( 25.6 % , 88 of 344 )\n",
      "‚úîÔ∏è Train Label 3: O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Train Sentence 4: The `` corrected '' cesarean rate ( maternal-fetal medicine and transported patients excluded ) was 12.4 % ( 273 of 2194 ) , and the `` corrected '' primary rate was 9.6 % ( 190 of 1975 )\n",
      "‚úîÔ∏è Train Label 4: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Train Sentence 5: Arrest of dilation was the most common indication in both `` corrected '' subgroups ( 23.4 and 24.6 % , respectively )\n",
      "‚úîÔ∏è Train Label 5: O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_item in range(5):\n",
    "    print(f\"‚úîÔ∏è Train Sentence {each_item+1}: {train_sentences[each_item]}\")\n",
    "    print(f\"‚úîÔ∏è Train Label {each_item+1}: {train_labels[each_item]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a27af66-d9b0-46f7-8e86-100a0abe15ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Test Sentence 1: Furthermore , when all deliveries were analyzed , regardless of risk status but limited to gestational age > or = 36 weeks , the rates did not change ( 12.6 % , 280 of 2214 ; primary 9.2 % , 183 of 1994 )\n",
      "‚úîÔ∏è Test Label 1: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Test Sentence 2: As the ambient temperature increases , there is an increase in insensible fluid loss and the potential for dehydration\n",
      "‚úîÔ∏è Test Label 2: O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Test Sentence 3: The daily high temperature ranged from 71 to 104 degrees F and AFI values ranged from 1.7 to 24.7 cm during the study period\n",
      "‚úîÔ∏è Test Label 3: O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Test Sentence 4: There was a significant correlation between the 2- , 3- , and 4-day mean temperature and AFI , with the 4-day mean being the most significant ( r = 0.31 , p & # 60 ; 0.001 )\n",
      "‚úîÔ∏è Test Label 4: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "\n",
      "‚úîÔ∏è Test Sentence 5: Fluctuations in ambient temperature are inversely correlated to changes in AFI\n",
      "‚úîÔ∏è Test Label 5: O O O O O O O O O O O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_item in range(5):\n",
    "    print(f\"‚úîÔ∏è Test Sentence {each_item+1}: {test_sentences[each_item]}\")\n",
    "    print(f\"‚úîÔ∏è Test Label {each_item+1}: {test_labels[each_item]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef38ada6-33f5-4edc-8dc5-8ba36b9ace59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1: Task 2 ‚Äì Count Number of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435d6c9c-43f2-4a0d-943c-331d204d75b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Number of sentences in processed train dataset is: 2599\n",
      "‚úîÔ∏è Number of sentences in processed test dataset is: 1056\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úîÔ∏è Number of sentences in processed train dataset is: {len(train_sentences)}\")\n",
    "print(f\"‚úîÔ∏è Number of sentences in processed test dataset is: {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b0e003-5ba2-414b-a798-5fc8d0e874b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1: Task 3 ‚Äì Count Number of Label Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c99ab8fe-fad1-4516-9ab2-0b1028fefbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Number of lines of labels in processed train dataset is: 2599\n",
      "‚úîÔ∏è Number of lines of labels in processed test dataset is: 1056\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úîÔ∏è Number of lines of labels in processed train dataset is: {len(train_labels)}\")\n",
    "print(f\"‚úîÔ∏è Number of lines of labels in processed test dataset is: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c73583a2-127b-4d02-a259-7714aca5a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Sentence and label counts match for both train and test datasets.\n"
     ]
    }
   ],
   "source": [
    "assert len(train_sentences)==len(train_labels),\"Mismatch in train sentences and labels!\"\n",
    "assert len(test_sentences)==len(test_labels),\"Mismatch in test sentences and labels!\"\n",
    "print(\"‚úîÔ∏è Sentence and label counts match for both train and test datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9c1d68-fc80-4d1d-afd1-b85e2326cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Sentence-label alignment looks good!\n"
     ]
    }
   ],
   "source": [
    "#Check the data alignment\n",
    "assert len(train_labels)==len(train_sentences),\"Mismatch between train sentences and labels!\"\n",
    "assert len(test_labels)==len(test_sentences),\"Mismatch between test sentences and labels!\"\n",
    "print(\"‚úîÔ∏è Sentence-label alignment looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4a030-916d-4c03-b826-6801047c16ed",
   "metadata": {},
   "source": [
    "### üöÄ Stage 2 ‚Äì Syntactic Tagging and Frequency Analysis\n",
    "\n",
    "- Task 2.1: Extract and Count NOUN/PROPN\n",
    "- Task 2.2: Show Top 25 Most Common NOUN/PROPN Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a86d742a-808e-4c07-9947-82de240fb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 2: Task 1 - List of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56d0004b-4db7-4c66-9c2c-b1630669e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of tokens NOUN/PROPER NOUN\n",
    "token_noun_propn=[]\n",
    "\n",
    "#List of NOUN/PROPER NOUN tokens\n",
    "for sentences in (train_sentences, test_sentences):\n",
    "    for sent in sentences:\n",
    "        processed_sent=nlp(sent)\n",
    "        for each_token in processed_sent:\n",
    "            if each_token.pos_==\"NOUN\" or each_token.pos_==\"PROPN\":\n",
    "                token_noun_propn.append(each_token.text)\n",
    "\n",
    "#Series of NOUN/PROPER NOUN tokens\n",
    "series_noun_propn=pd.Series(token_noun_propn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ee79ec-4809-4478-a04a-d8256c9a439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 2: Task 2 - Top 25 NOUN/PROPER NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4198f503-95e1-47f0-b0b9-bed59cb27969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Top 25 NOUN/PROP NOUN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "patients        492\n",
       "treatment       281\n",
       "%               247\n",
       "cancer          200\n",
       "therapy         175\n",
       "study           153\n",
       "disease         141\n",
       "cell            140\n",
       "lung            116\n",
       "group            94\n",
       "chemotherapy     88\n",
       "gene             87\n",
       "effects          85\n",
       "women            77\n",
       "results          77\n",
       "use              73\n",
       "cases            71\n",
       "risk             71\n",
       "surgery          71\n",
       "analysis         70\n",
       "rate             67\n",
       "response         66\n",
       "survival         65\n",
       "children         64\n",
       "dose             63\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 25 NOUN/PROPER NOUN\n",
    "print(\"‚úîÔ∏è Top 25 NOUN/PROP NOUN:\") \n",
    "series_noun_propn.value_counts().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6309-6004-469b-b053-73d0a2a0ef7b",
   "metadata": {},
   "source": [
    "### üöÄ Stage 3 ‚Äì Feature Engineering for CRF\n",
    "- Define Features Including PoS Tag\n",
    "- Add Previous Word's Info\n",
    "- Mark Sentence Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4049ff8d-6422-4b92-be6f-693ab6ee8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "def feature_engineering(sentence, pos, pos_tags):\n",
    "  word=sentence[pos]\n",
    "  features=[\n",
    "    'word.lower='+word.lower(),\n",
    "    'word[-3:]='+word[-3:],\n",
    "    'word[-2:]='+word[-2:],\n",
    "    'word.isupper=%s'%word.isupper(),\n",
    "    'word.isdigit=%s'%word.isdigit(),\n",
    "    'word.startsWithCapital=%s'%word[0].isupper(),\n",
    "    'word.pos='+ pos_tags[pos]\n",
    "  ]\n",
    "#Defining Feature\n",
    "  if(pos>0):\n",
    "    prev_word=sentence[pos-1]\n",
    "    features.extend([\n",
    "    'prev_word.lower='+prev_word.lower(), \n",
    "    'prev_word.isupper=%s'%prev_word.isupper(),\n",
    "    'prev_word.isdigit=%s'%prev_word.isdigit(),\n",
    "    'prev_word.startsWithCapital=%s'%prev_word[0].isupper(),\n",
    "    'prev_word.pos='+pos_tags[pos-1]\n",
    "  ])\n",
    "  else:\n",
    "    features.append('BEG')\n",
    "  if(pos==len(sentence)-1):\n",
    "    features.append('END')\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172e7d9-6cf8-49b8-b261-72cf9b3afcfc",
   "metadata": {},
   "source": [
    "### üöÄ Stage 4 ‚Äì Feature + Label Preparation\n",
    "- Compute features for all sentences\n",
    "- Extract labels for each token in sentence-aligned format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c58ffa9e-6044-45d9-8cfa-af9a027efd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature for a sentence\n",
    "def sentence_feature(sentence):\n",
    "    processed_sent=nlp(sentence)\n",
    "    postags=[]\n",
    "    for each_token in processed_sent:\n",
    "        postags.append(each_token.pos_)\n",
    "    sentence_list=sentence.split()\n",
    "    return [feature_engineering(sentence_list,pos,postags) for pos in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f1b3d7b-8514-4f71-9d37-67f7c3fb9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence lables\n",
    "def label_feature(labels):\n",
    "  return labels.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb078d94-05c8-4366-8b52-b6c838a95316",
   "metadata": {},
   "source": [
    "### üöÄ Stage 5 ‚Äì Prepare CRF Model Inputs\n",
    "- Task 5.1: X_train and X_test already contain feature dicts\n",
    "- Task 5.2: y_train and y_test already contain label sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a845b857-61b9-495f-a5b6-af9182f7d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[sentence_feature(sentence) for sentence in train_sentences]\n",
    "X_test=[sentence_feature(sentence) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e4d0df3-0927-4b9f-b432-4dc314a72841",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=[label_feature(labels) for labels in train_labels]\n",
    "Y_test=[label_feature(labels) for labels in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd453522-687e-4d84-9ccd-f9ba4ff421ac",
   "metadata": {},
   "source": [
    "### üöÄ Stage 6 - Train CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa425069-57cb-4687-8bfd-752d6490e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build CRF model\n",
    "crf=sklearn_crfsuite.CRF(max_iterations=100)\n",
    "try:\n",
    "    crf.fit(X_train, Y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688fc1a-7702-4fec-a811-6c4de16d5948",
   "metadata": {},
   "source": [
    "### üöÄ Stage 7 ‚Äì Predict + Evaluate\n",
    "- Task 7.1: Predict labels for each sentence in the test set\n",
    "- Task 7.2: Calculate F1-score using actual vs predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2af4312c-555f-4263-b44d-752ba6e5ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52a80361-8d56-4c88-803c-b03cfdc0259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è F1 score is: 0.9071\n"
     ]
    }
   ],
   "source": [
    "f1_score=metrics.flat_f1_score(Y_test,Y_pred,average='weighted')\n",
    "print(f\"‚úîÔ∏è F1 score is: {round(f1_score,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96714c02-be18-446f-9726-657948baca66",
   "metadata": {},
   "source": [
    "### üöÄ Stage 8 - Model Evaluation\n",
    "\n",
    "- Extract Treatments (T) Corresponding to Diseases (D) in Test Data\n",
    "- Process the X_test, y_pred, and original sentences to extract treatments that are nearby each disease mention.\n",
    "- Identify Entities from Predicted BIO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "597aaa30-6e53-4c91-a509-f255c4ed2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary for Diseases and their corresponding Treatments\n",
    "Disease_Treat_dict=dict()\n",
    "for i in range(len(Y_pred)):\n",
    "#Predicted labels of each test sentence into 'val'\n",
    "    val=Y_pred[i]\n",
    "#Empty strings for Diseases and Treatments\n",
    "    Diseases=\"\"\n",
    "    Treatments=\"\"\n",
    "#Loop to iterate labels and map Diseases and Treatments\n",
    "    for j in range(len(val)):\n",
    "        if val[j]=='D':\n",
    "            Diseases+=test_sentences[i].split()[j]+\" \"\n",
    "        elif val[j]=='T':\n",
    "            Treatments+= test_sentences[i].split()[j]+\" \"\n",
    "    Diseases=Diseases.lstrip().rstrip()\n",
    "    Treatments=Treatments.lstrip().rstrip()\n",
    "#Ignore blank, add Disease as per Treatment or Append as required\n",
    "    if Diseases !=\"\" and Treatments !=\"\":\n",
    "        if Diseases in Disease_Treat_dict.keys():\n",
    "            treat_out=list(Disease_Treat_dict[Diseases])\n",
    "            treat_out.append(Treatments)\n",
    "            Disease_Treat_dict[Diseases]=treat_out\n",
    "        elif Diseases not in Disease_Treat_dict.keys():\n",
    "            Disease_Treat_dict[Diseases]=Treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc30c7f5-182b-4b7c-a631-a9e2032135ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Predicted treatments for 'hereditary retinoblastoma': radiotherapy\n"
     ]
    }
   ],
   "source": [
    "target_disease='hereditary retinoblastoma'\n",
    "print(f\"‚úîÔ∏è Predicted treatments for '{target_disease}': {Disease_Treat_dict['hereditary retinoblastoma']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc8d60-2181-4827-bd4d-3ea837027ebd",
   "metadata": {},
   "source": [
    "### üöÄ Model Evaluation Summary\n",
    "#### Dataset Overview\n",
    "- Total sentences in training set: 2,599\n",
    "- Total sentences in test set: 1,056\n",
    "- Corresponding lines of entity labels align perfectly with the sentence counts, indicating clean and consistent dataset preparation.\n",
    "\n",
    "#### Corpus Insights\n",
    "- The top 25 most frequent nouns and proper nouns highlight domain relevance, with high occurrences of terms like patients, treatment, cancer, therapy, and disease.\n",
    "- This reinforces the biomedical focus of the corpus and the richness of vocabulary in the clinical treatment-disease context.\n",
    "\n",
    "#### Model Performance\n",
    "- The CRF-based Named Entity Recognition (NER) model achieved a weighted F1-score of 0.9071, demonstrating high accuracy in tagging disease and treatment entities across the test set.\n",
    "- This indicates the model‚Äôs strong ability to generalize and correctly identify biomedical entities even in unseen data.\n",
    "\n",
    "#### Use Case Validation\n",
    "\n",
    "In a targeted evaluation, the model correctly identified the treatment ‚Äúradiotherapy‚Äù associated with the disease ‚Äúhereditary retinoblastoma‚Äù, showcasing its practical applicability in extracting clinically relevant information from natural language text.\n",
    "\n",
    "### üìå Conclusion\n",
    "The CRF-based NER model has proven to be robust and reliable in identifying diseases and treatments within biomedical text. With a high F1 score and accurate predictions on domain-specific cases, it is well-suited for downstream tasks such as automated medical literature mining, treatment recommendation pipelines, and clinical decision support systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
